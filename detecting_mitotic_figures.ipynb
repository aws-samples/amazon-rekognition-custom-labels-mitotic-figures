{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting mitotic figures using Amazon Rekognition Custom Labels\n",
    "\n",
    "Mitotic figures are cells that are dividing via a process called _mitosis_ to create two new cells. Identifying and counting these mitotic figures is part of histopathology tissue analysis, considered the gold standard in cancer diagnosis. A pathologist will usually take hematoxylin-eosin stained tissue samples and identify these and other features when evaluating tumors.\n",
    "\n",
    "This process depends entirely on pathologists and is costly and time consuming. As technology evolves, whole-slide imaging (WSI) techniques have enabled laboratories to start scanning and digitizing samples. And with the recent advances in machine learning (ML), it has now become feasible to build systems that can help pathologists by automatic the detection of abnormal and/or relevant features in pathology slides.\n",
    "\n",
    "In this workshop, we will explore how Amazon Rekognition Custom Labels can be used to implement such automated detection systems by processing WSI data, and using it to train a custom model that detects mitotic figures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you continue\n",
    "\n",
    "Make sure that you are using the _Python 3 (Data Science)_ kernel, and an `ml.m5.large` instance (will show up as 2 vCPU + 8 GiB on toolbar). Using a smaller instance may cause some operations to run out of memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies\n",
    "\n",
    "To prepare our SageMaker Studio application instance, we will update system packages first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt update > /dev/null && sudo apt dist-upgrade -y > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the WSI data, we need the [OpenSlide](https://openslide.org) library and tooling, which we can install using `apt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo apt install -y build-essential openslide-tools python3-openslide libgl1-mesa-glx > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also use [SlideRunner](https://github.com/DeepPathology/SlideRunner) and [fastai](https://fast.ai) to load and process the slides, which we need to install by using `pip`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install SlideRunner SlideRunner_dataAccess fastai==1.0.61 > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading the dataset\n",
    "\n",
    "We will use the MITOS_WSI_CMC dataset, which is available on [GitHub](https://github.com/DeepPathology/MITOS_WSI_CMC). Images are downloaded from Figshare.\n",
    "\n",
    "This step takes approximately 10-12 minutes. If you are not running this as a self-paced lab, your instructor will make a pause here, and introduce you to other necessary concepts while waiting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import download_dataset\n",
    "download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "In the previous step, you downloaded the WSI files from which you will generate the training and test images for Amazon Rekognition. However, you still need the labels for each of the mitotic figures in those images. These are stored in a sqlite database that is the dataset's repository. We will download the database now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import hashlib\n",
    "import os\n",
    "from typing import List\n",
    "import urllib\n",
    "\n",
    "import numpy as np\n",
    "from SlideRunner.dataAccess.database import Database\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "DATABASE_URL = 'https://github.com/DeepMicroscopy/MITOS_WSI_CMC/raw/refs/heads/master/databases/MITOS_WSI_CMC_MEL.sqlite'\n",
    "DATABASE_FILENAME = 'MITOS_WSI_CMC_MEL.sqlite'\n",
    "DATABASE_SHA256_SIGNATURE = 'a328051d0c4fb4797f838cb1cf919f3ff0acdb6ac56cfe1c6e063fac25f25302'\n",
    "\n",
    "Path(\"./databases\").mkdir(parents=True, exist_ok=True)\n",
    "path_to_database_file = os.path.join('databases', DATABASE_FILENAME)\n",
    "\n",
    "local_filename, headers = urllib.request.urlretrieve(\n",
    "    DATABASE_URL,\n",
    "    filename=path_to_database_file,\n",
    ")\n",
    "\n",
    "# Verify the database file\n",
    "sha256 = hashlib.sha256()\n",
    "\n",
    "with open(path_to_database_file, 'rb') as database_file:\n",
    "    while True:\n",
    "        data = database_file.read(65536)\n",
    "        if not data:\n",
    "            break\n",
    "\n",
    "        sha256.update(data)\n",
    "\n",
    "if DATABASE_SHA256_SIGNATURE != sha256.hexdigest():\n",
    "    print(\"Incorrect SHA256 signature. Database might have been tampered with! Erasing...\")\n",
    "    os.remove(path_to_database_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "There are a few things we still need to define before moving on:\n",
    "\n",
    "### Storage\n",
    "\n",
    "We need an Amazon S3 bucket to place the image files, so that Amazon Rekognition can read those during training and testing. We will use the default Amazon SageMaker bucket that is automatically created for you.\n",
    "\n",
    "### Database\n",
    "\n",
    "To have access to the annotations, we need to open the database using `SlideRunner`.\n",
    "\n",
    "### Test slides\n",
    "\n",
    "We need to define a set of test slides to set apart. These will be used to assess your model's ability to generalize, and thus cannot be used to generate training data. That is the reason we are defining them beforehand.\n",
    "\n",
    "There are three different arrays with different test slides defined for each. By default, the first set of test slides is used, by you can go ahead and try different combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "sm_session = sagemaker.Session()\n",
    "\n",
    "size=512\n",
    "bucket_name = sm_session.default_bucket()\n",
    "\n",
    "database = Database()\n",
    "database.open(os.path.join('databases', DATABASE_FILENAME))\n",
    "\n",
    "slidelist_test_1 = ['14','18','3','22','10','15','21']\n",
    "slidelist_test_2 = ['1','20','17','5','2','11','16']\n",
    "slidelist_test_3 = ['13','7','19','8','6','9', '12']\n",
    "slidelist_test = slidelist_test_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the slides\n",
    "\n",
    "Now we can call the `get_slides` function, which will produce a list of training and test slides we can use to generate the training and test images. The code for this function is in the `sampling.py` file.\n",
    "\n",
    "We need to pass:\n",
    "* A reference to the database object, so that annotations can be read and linked to the slides.\n",
    "* A list of slides to use to generate the test dataset (and to exclude from the training dataset).\n",
    "* The ID of the negative class - Not used in this workshop.\n",
    "* The size (both width and height), in pixels, of the image that is generated when `get_patch` is invoked on a `SlideContainer`. This effectively sets the size of the image that is created for Amazon Rekognition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sampling import get_slides\n",
    "\n",
    "image_size = 512\n",
    "\n",
    "lbl_bbox, training_slides, test_slides, files = get_slides(database, slidelist_test, negative_class=1, size=image_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle the slides\n",
    "\n",
    "We want to randomly sample from the training and test slides. Using the lists of training and test slides, we will randomly select `n_training_images` times a file for training, and `n_test_images` times a file for test. Notice that we have chosen to have a test set that contains 20% the number of images the training set has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_training_images = 500\n",
    "n_test_images = int(0.2 * n_training_images)\n",
    "\n",
    "training_files = list([\n",
    "    (y, files[y]) for y in np.random.choice(\n",
    "        [x for x in training_slides], n_training_images)\n",
    "])\n",
    "test_files = list([\n",
    "    (y, files[y]) for y in np.random.choice(\n",
    "        [x for x in test_slides], n_test_images)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the images for training the Rekognition Custom Labels model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"rek_slides/training\").mkdir(parents=True, exist_ok=True)\n",
    "Path(\"rek_slides/test\").mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to build JSON lines manifest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_annotation_json_line(filename, channel, annotations, labels):\n",
    "    \n",
    "    objects = list([{'confidence' : 1} for i in range(0, len(annotations))])\n",
    "    \n",
    "    return json.dumps({\n",
    "        'source-ref': f's3://{bucket_name}/data/{channel}/{filename}',\n",
    "        'bounding-box': {\n",
    "            'image_size': [{\n",
    "                'width': size,\n",
    "                'height': size,\n",
    "                'depth': 3\n",
    "            }],\n",
    "            'annotations': annotations,\n",
    "        },\n",
    "        'bounding-box-metadata': {\n",
    "            'objects': objects,\n",
    "            'class-map': dict({ x: str(x) for x in labels }),\n",
    "            'type': 'groundtruth/object-detection',\n",
    "            'human-annotated': 'yes',\n",
    "            'creation-date': datetime.datetime.now().isoformat(),\n",
    "            'job-name': 'rek-pathology',\n",
    "        }\n",
    "    })\n",
    "\n",
    "def generate_annotations(x_start: int, y_start: int, bboxes, labels, filename: str, channel: str):\n",
    "    annotations = []\n",
    "    \n",
    "    for bbox in bboxes:\n",
    "        if check_bbox(x_start, y_start, bbox):\n",
    "            # Get coordinates relative to this slide.\n",
    "            x0 = bbox.left - x_start\n",
    "            y0 = bbox.top - y_start\n",
    "            \n",
    "            annotation = {\n",
    "                'class_id': 1,\n",
    "                'top': y0,\n",
    "                'left': x0,\n",
    "                'width': bbox.right - bbox.left,\n",
    "                'height': bbox.bottom - bbox.top\n",
    "            }\n",
    "            \n",
    "            annotations.append(annotation)\n",
    "    \n",
    "    return get_annotation_json_line(filename, channel, annotations, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we get random pieces of our images to use for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import json\n",
    "import random\n",
    "\n",
    "from fastai import *\n",
    "from fastai.vision import *\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "# Margin size, in pixels, for training images. This is the space we leave on\n",
    "# each side for the bounding box(es) to be well into the image.\n",
    "margin_size = 64\n",
    "\n",
    "training_annotations = []\n",
    "test_annotations = []\n",
    "\n",
    "def check_bbox(x_start: int, y_start: int, bbox) -> bool:\n",
    "    return (bbox._left > x_start and\n",
    "            bbox._right < x_start + image_size and\n",
    "            bbox._top > y_start and\n",
    "            bbox._bottom < y_start + image_size)\n",
    "\n",
    "\n",
    "def generate_images(file_list) -> None:\n",
    "    for f_idx in tqdm(range(0, len(file_list)), desc='Writing training images...'):\n",
    "        slide_idx, f = file_list[f_idx]\n",
    "        bboxes = lbl_bbox[slide_idx][0]\n",
    "        labels = lbl_bbox[slide_idx][1]\n",
    "\n",
    "        # Calculate the minimum and maximum horizontal and vertical positions\n",
    "        # that bounding boxes should have within the image.\n",
    "        x_min = min(map(lambda x: x.left, bboxes)) - margin_size\n",
    "        y_min = min(map(lambda x: x.top, bboxes)) - margin_size\n",
    "        x_max = max(map(lambda x: x.right, bboxes)) + margin_size\n",
    "        y_max = max(map(lambda x: x.bottom, bboxes)) + margin_size\n",
    "\n",
    "        result = False\n",
    "        while not result:\n",
    "            x_start = random.randint(x_min, x_max - image_size)\n",
    "            y_start = random.randint(y_min, y_max - image_size)\n",
    "\n",
    "            for bbox in bboxes:\n",
    "                if check_bbox(x_start, y_start, bbox):\n",
    "                    result = True\n",
    "                    break\n",
    "\n",
    "        filename = f'slide_{f_idx}.png'\n",
    "        channel = 'test' if slide_idx in test_slides else 'training'\n",
    "        annotation = generate_annotations(x_start, y_start, bboxes, labels, filename, channel)\n",
    "\n",
    "        if channel == 'training':\n",
    "            training_annotations.append(annotation)\n",
    "        else:\n",
    "            test_annotations.append(annotation)\n",
    "\n",
    "        img = Image(pil2tensor(f.get_patch(x_start, y_start) / 255., np.float32))\n",
    "        img.save(f'rek_slides/{channel}/{filename}')\n",
    "\n",
    "generate_images(training_files)\n",
    "generate_images(test_files)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write the manifest files to disk\n",
    "\n",
    "The previous cell generated a series of annotations in the Amazon SageMaker Ground Truth format, which is the same Amazon Rekognition expects. The specifics for object detection are detailed [in the documentation](https://docs.aws.amazon.com/rekognition/latest/customlabels-dg/cd-manifest-files-object-detection.html).\n",
    "\n",
    "Annotations were stored in the `training_annotations` and `test_annotations` lists. Now, we need to write a `manifest.json` file with the contents of each list into the _training_ and _test_ directories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rek_slides/training/manifest.json', 'w') as mf:\n",
    "    mf.write(\"\\n\".join(training_annotations))\n",
    "\n",
    "with open('rek_slides/test/manifest.json', 'w') as mf:\n",
    "    mf.write(\"\\n\".join(test_annotations))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer the files to S3\n",
    "\n",
    "Having written the images and the manifest file, we can now upload everything to our S3 bucket. We will use the `upload_data` method exposed by the SageMaker `Session` object, which recursively uploads the contents of a directory to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "\n",
    "\n",
    "sm_session = sagemaker.Session()\n",
    "data_location = sm_session.upload_data(\n",
    "    './rek_slides',\n",
    "    bucket=bucket_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an Amazon Rekognition Custom Labels project\n",
    "\n",
    "With our data already in S3, we can take the first step towards training a custom model, and create a Custom Labels project. Using the `boto3` library, we create an Amazon Rekognition client and invoke the `create_project` method. This method only takes a project name as input. If succesful, it returns the ARN (Amazon Resource Name) of the newly created project, which we need to save for future use.\n",
    "\n",
    "If you already created the project and just want to retrieve its ARN, you can use the `describe_projects` method exposed by the Amazon Rekognition client, and then retrieve the ARN from the list of projects returned. The commented line assumes that you only have one project and retrieves the ARN from the first description in the list. If you are doing this as a self-paced lab and have previously used Rekognition, be aware that using the zero index may not retrieve the ARN of your workshop project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "\n",
    "project_name = 'rek-mitotic-figures-workshop'\n",
    "\n",
    "rek = boto3.client('rekognition')\n",
    "response = rek.create_project(ProjectName=project_name)\n",
    "\n",
    "# If you have already created the project, use the describe_projects call to\n",
    "# retrieve the project ARN.\n",
    "# response = rek.describe_projects()['ProjectDescriptions'][0]\n",
    "\n",
    "project_arn = response['ProjectArn']\n",
    "project_arn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a project version\n",
    "\n",
    "To create a project version, we need to specify:\n",
    "* The name of the version.\n",
    "* The name of the bucket, along with a prefix under which you want the training results to be stored.\n",
    "* Test and a training datasets.\n",
    "\n",
    "For the test and training datasets, you need to tell Amazon Rekognition where you training and test images are stored. The information is contained in the `manifest.json` files that we created in an earlier step, and all we need to do know is to indicate where they are stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version_name = '1'\n",
    "\n",
    "output_config = {\n",
    "    'S3Bucket': bucket_name,\n",
    "    'S3KeyPrefix': 'output',\n",
    "}\n",
    "\n",
    "training_dataset = {\n",
    "    'Assets': [\n",
    "        {\n",
    "            'GroundTruthManifest': {\n",
    "                'S3Object': {\n",
    "                    'Bucket': bucket_name,\n",
    "                    'Name': 'data/training/manifest.json'\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    "}\n",
    "\n",
    "testing_dataset = {\n",
    "    'Assets': [\n",
    "        {\n",
    "            'GroundTruthManifest': {\n",
    "                'S3Object': {\n",
    "                    'Bucket': bucket_name,\n",
    "                    'Name': 'data/test/manifest.json'\n",
    "                }\n",
    "            },\n",
    "        },\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also define a helper function to describe the different versions of a project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def describe_project_versions():\n",
    "    describe_response = rek.describe_project_versions(\n",
    "        ProjectArn=project_arn,\n",
    "        VersionNames=[version_name],\n",
    "    )\n",
    "\n",
    "    for model in describe_response['ProjectVersionDescriptions']:\n",
    "        print(f\"Status: {model['Status']}\")\n",
    "        print(f\"Message: {model['StatusMessage']}\")\n",
    "    \n",
    "    return describe_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All that is left to do is to invoke the `create_project_version` method with the parameters we just defined. Calling this method start the task of training a model asynchronously. To wait for the task to finish, we create a _waiter_, which will poll the service periodically and exit once the model has either been successfully trained, or an error has occurred."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = rek.create_project_version(\n",
    "    VersionName=version_name,\n",
    "    ProjectArn=project_arn,\n",
    "    OutputConfig=output_config,\n",
    "    TrainingData=training_dataset,\n",
    "    TestingData=testing_dataset,\n",
    ")\n",
    "\n",
    "waiter = rek.get_waiter('project_version_training_completed')\n",
    "waiter.wait(\n",
    "    ProjectArn=project_arn,\n",
    "    VersionNames=[version_name],\n",
    ")\n",
    "\n",
    "describe_response = describe_project_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the model\n",
    "\n",
    "If you got this far, it means that your project is ready to run! Before you can start doing inference with your Amazon Rekognition Custom Labels model, you need to start the model.\n",
    "\n",
    "### Start the model\n",
    "\n",
    "To start the model, simply call the `start_project_version` method. You will need to provide two parameters:\n",
    "* Your project version ARN.\n",
    "* A number of inference units.\n",
    "\n",
    "The number of inference units is related to the amount of resources deployed for your model. The higher the number of inference units you allocate, the higher the throughput you can achieve. However, since you are billed based on the number of inference units as well, the higher the cost.\n",
    "\n",
    "The model can take a 5-15 minutes to deploy. If doing this as an instructor-led workshop, your instructor will use this time to answer questions or deliver additional content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_arn = describe_response['ProjectVersionDescriptions'][0]['ProjectVersionArn']\n",
    "\n",
    "response = rek.start_project_version(\n",
    "    ProjectVersionArn=model_arn,\n",
    "    MinInferenceUnits=1,\n",
    ")\n",
    "waiter = rek.get_waiter('project_version_running')\n",
    "waiter.wait(\n",
    "    ProjectArn=project_arn,\n",
    "    VersionNames=[version_name],\n",
    ")\n",
    "\n",
    "describe_project_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submit an image for inference\n",
    "\n",
    "Our trained model is now ready for inference. Use any of the files in the `rek_slides/test` and send it over to your endpoint by using the `detect_custom_labels` method of the SDK to see how your model is now able to detect mitotic figures in microscopy images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "\n",
    "# We'll use one of our test images to try out our model.\n",
    "with open('./rek_slides/test/slide_0.png', 'rb') as image_file:\n",
    "    image_bytes=image_file.read()\n",
    "\n",
    "\n",
    "# Send the image data to the model.\n",
    "response = rek.detect_custom_labels(\n",
    "    ProjectVersionArn=model_arn,\n",
    "    Image={\n",
    "        'Bytes': image_bytes\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# \n",
    "img = Image.open(io.BytesIO(image_bytes))\n",
    "draw = ImageDraw.Draw(img)\n",
    "\n",
    "for custom_label in response['CustomLabels']:\n",
    "    geometry = custom_label['Geometry']['BoundingBox']\n",
    "    w = geometry['Width'] * img.width\n",
    "    h = geometry['Height'] * img.height\n",
    "    l = geometry['Left'] * img.width\n",
    "    t = geometry['Top'] * img.height\n",
    "    draw.rectangle([l, t, l + w, t + h], outline=(0, 0, 255, 255), width=5)\n",
    "\n",
    "plt.imshow(np.asarray(img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up\n",
    "\n",
    "To finish this workshop, we will stop the model.\n",
    "\n",
    "**Do not forget to run this step when you complete there workshop. Custom Labels models are billed by the minute.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rek.stop_project_version(\n",
    "    ProjectVersionArn=model_arn,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "instance_type": "ml.m5.large",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
